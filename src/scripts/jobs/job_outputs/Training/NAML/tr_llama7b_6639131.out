============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
[00:00:00] |Worker| START TIME: 2024-06-14 16:01:55.206568
[00:00:00] |Worker| python  worker.py --data config/data/mind-llama.yaml --embed config/embed/llama-token.yaml --model config/model/llm/llama-naml.yaml --exp config/exp/tt-llm.yaml --embed_hidden_size 4096 --llm_ver 7b --layer 0 --version small --lr 0.0001 --item_lr 0.00001 --batch_size 32 --acc_batch 2 --epoch_batch -4
[00:00:00] |Worker| {
    "data": {
        "name": "ebnerd_small_tokenized-Llama",
        "base_dir": "/home/scur1569/Legommenders/ebnerd_small_tokenized",
        "item": {
            "filter_cache": true,
            "depot": "/home/scur1569/Legommenders/ebnerd_small_tokenized/news-llama",
            "order": [
                "title",
                "cat"
            ],
            "append": [
                "nid"
            ]
        },
        "user": {
            "filter_cache": true,
            "depots": {
                "train": {
                    "path": "/home/scur1569/Legommenders/ebnerd_small_tokenized/train"
                },
                "dev": {
                    "path": "/home/scur1569/Legommenders/ebnerd_small_tokenized/dev"
                }
            },
            "filters": {
                "history": [
                    "x"
                ]
            },
            "union": [
                "/home/scur1569/Legommenders/ebnerd_small_tokenized/user"
            ],
            "candidate_col": "nid",
            "clicks_col": "history",
            "label_col": "click",
            "group_col": "imp",
            "user_col": "uid",
            "index_col": "index"
        }
    },
    "embed": {
        "name": "llama-token",
        "embeddings": [
            {
                "vocab_name": "llama",
                "vocab_type": "numpy",
                "path": "ebnerd_small_tokenized/llama-token2.npy",
                "frozen": true
            }
        ]
    },
    "model": {
        "name": "LLAMA-NAML.D64.L0.Lora1",
        "meta": {
            "item": "Llama",
            "user": "Ada",
            "predictor": "Dot"
        },
        "config": {
            "use_neg_sampling": false,
            "use_item_content": true,
            "max_item_content_batch_size": 0,
            "same_dim_transform": false,
            "embed_hidden_size": 4096,
            "hidden_size": 64,
            "neg_count": null,
            "item_config": {
                "llm_dir": "/home/scur1569/Legommenders/llama-7b",
                "layer_split": 0,
                "lora": 1,
                "weights_dir": "data/ebnerd_small_tokenized-Llama/llama-7b-split"
            },
            "user_config": {
                "num_attention_heads": 12,
                "inputer_config": {
                    "use_cls_token": false,
                    "use_sep_token": false
                }
            }
        }
    },
    "exp": {
        "name": "train_test",
        "dir": "saving/ebnerd_small_tokenized-Llama/LLAMA-NAML.D64.L0.Lora1/llama-token-train_test",
        "log": "saving/ebnerd_small_tokenized-Llama/LLAMA-NAML.D64.L0.Lora1/llama-token-train_test/exp.log",
        "mode": "train_test",
        "load": {
            "save_dir": null,
            "epochs": null,
            "model_only": true,
            "strict": false,
            "wait": false
        },
        "store": {
            "metric": "GAUC",
            "maximize": true,
            "top": 1,
            "early_stop": 2
        },
        "policy": {
            "epoch_start": 0,
            "epoch": 50,
            "lr": 0.0001,
            "item_lr": 1e-05,
            "freeze_emb": false,
            "pin_memory": false,
            "epoch_batch": -4.0,
            "batch_size": 32,
            "accumulate_batch": 2,
            "device": "gpu",
            "n_warmup": 0,
            "check_interval": -2,
            "simple_dev": false
        },
        "metrics": [
            "GAUC",
            "MRR",
            "NDCG@1",
            "NDCG@5",
            "NDCG@10"
        ]
    },
    "embed_hidden_size": 4096,
    "llm_ver": "7b",
    "layer": 0,
    "version": "small",
    "lr": 0.0001,
    "item_lr": 1e-05,
    "batch_size": 32,
    "acc_batch": 2,
    "epoch_batch": -4.0,
    "warmup": 0,
    "fast_eval": true,
    "simple_dev": false,
    "lora": 1,
    "lora_r": 32,
    "mind_large_submission": false,
    "hidden_size": 64,
    "max_item_batch_size": 0,
    "page_size": 512,
    "patience": 2,
    "epoch_start": 0,
    "frozen": true,
    "load_path": null,
    "rand": {},
    "time": {},
    "seed": 2023
}
[00:00:00] |GPU| choose 0 GPU with 40329 / 40960 MB
MODES
{'train', 'test', 'dev'}
[00:00:00] |Controller| dataset type:  news
[00:00:00] |Controller| build column map ...
path /home/scur1569/Legommenders/ebnerd_small_tokenized/train
Phases.train is in modes
DepotHub.get called with path: /home/scur1569/Legommenders/ebnerd_small_tokenized/train, filter_cache: True
loaded 2585747 samples from /home/scur1569/Legommenders/ebnerd_small_tokenized/train
[00:00:02] |CachingDep| load 1 filter caches on 
        UniDep (2.0): /home/scur1569/Legommenders/ebnerd_small_tokenized/train

        Sample Size: 2585747
        Id Column: index
        Columns:
        	index, vocab index (size 2585747)
        	imp, vocab imp (size 232887)
        	uid, vocab uid (size 18827)
        	nid, vocab nid (size 20739)
        	click, vocab click (size 2)

CachingDep initialized: 
        UniDep (2.0): /home/scur1569/Legommenders/ebnerd_small_tokenized/train

        Sample Size: 2585747
        Id Column: index
        Columns:
        	index, vocab index (size 2585747)
        	imp, vocab imp (size 232887)
        	uid, vocab uid (size 18827)
        	nid, vocab nid (size 20739)
        	click, vocab click (size 2)

Initialized train_depot: 
        UniDep (2.0): /home/scur1569/Legommenders/ebnerd_small_tokenized/train

        Sample Size: 2585747
        Id Column: index
        Columns:
        	index, vocab index (size 2585747)
        	imp, vocab imp (size 232887)
        	uid, vocab uid (size 18827)
        	nid, vocab nid (size 20739)
        	click, vocab click (size 2)

Phases.dev is in modes
DepotHub.get called with path: /home/scur1569/Legommenders/ebnerd_small_tokenized/dev, filter_cache: True
loaded 2928942 samples from /home/scur1569/Legommenders/ebnerd_small_tokenized/dev
[00:00:04] |CachingDep| load 1 filter caches on 
        UniDep (2.0): /home/scur1569/Legommenders/ebnerd_small_tokenized/dev

        Sample Size: 2928942
        Id Column: index
        Columns:
        	index, vocab index (size 2928942)
        	imp, vocab imp (size 244647)
        	uid, vocab uid (size 18827)
        	nid, vocab nid (size 20739)
        	click, vocab click (size 2)

CachingDep initialized: 
        UniDep (2.0): /home/scur1569/Legommenders/ebnerd_small_tokenized/dev

        Sample Size: 2928942
        Id Column: index
        Columns:
        	index, vocab index (size 2928942)
        	imp, vocab imp (size 244647)
        	uid, vocab uid (size 18827)
        	nid, vocab nid (size 20739)
        	click, vocab click (size 2)

Initialized dev_depot: 
        UniDep (2.0): /home/scur1569/Legommenders/ebnerd_small_tokenized/dev

        Sample Size: 2928942
        Id Column: index
        Columns:
        	index, vocab index (size 2928942)
        	imp, vocab imp (size 244647)
        	uid, vocab uid (size 18827)
        	nid, vocab nid (size 20739)
        	click, vocab click (size 2)

loaded 2928942 samples from /home/scur1569/Legommenders/ebnerd_small_tokenized/dev
modify sample_size to 18827
Initialized fast_eval_depot: 
        UniDep (2.0): /home/scur1569/Legommenders/ebnerd_small_tokenized/dev

        Sample Size: 18827
        Id Column: index
        Columns:
        	index, vocab index (size 2928942)
        	imp, vocab imp (size 244647)
        	uid, vocab uid (size 18827)
        	nid, vocab nid (size 20739)
        	click, vocab click (size 2)

DepotHub.get called with path: /home/scur1569/Legommenders/ebnerd_small_tokenized/user, filter_cache: False
loaded 18827 samples from /home/scur1569/Legommenders/ebnerd_small_tokenized/user
CachingDep initialized: 
        UniDep (2.0): /home/scur1569/Legommenders/ebnerd_small_tokenized/user

        Sample Size: 18827
        Id Column: uid
        Columns:
        	uid, vocab uid (size 18827)
        	history, vocab nid (size 20739), max length 30

DepotHub.get called with path: /home/scur1569/Legommenders/ebnerd_small_tokenized/user, filter_cache: False
Depot found in cache for path: /home/scur1569/Legommenders/ebnerd_small_tokenized/user
DepotHub.get called with path: /home/scur1569/Legommenders/ebnerd_small_tokenized/user, filter_cache: False
Depot found in cache for path: /home/scur1569/Legommenders/ebnerd_small_tokenized/user
[00:00:06] |Depots| Filter history with x in train phase, sample num: 2585747 -> 2585747
[00:00:06] |Depots| Filter history with x in dev phase, sample num: 2928942 -> 2928942
Initialized train_hub: <loader.data_hub.DataHub object at 0x15366ea44950>
Initialized dev_hub: <loader.data_hub.DataHub object at 0x15366ea45250>
Initialized fast_eval_hub: <loader.data_hub.DataHub object at 0x15366ea45550>
Initialized hubs: {'train': <loader.data_hub.DataHub object at 0x15366ea44950>, 'dev': <loader.data_hub.DataHub object at 0x15366ea45250>, 'test': None, 'fast_eval': <loader.data_hub.DataHub object at 0x15366ea45550>}
DepotHub.get called with path: /home/scur1569/Legommenders/ebnerd_small_tokenized/news-llama, filter_cache: False
loaded 20738 samples from /home/scur1569/Legommenders/ebnerd_small_tokenized/news-llama
CachingDep initialized: 
        UniDep (2.0): /home/scur1569/Legommenders/ebnerd_small_tokenized/news-llama

        Sample Size: 20738
        Id Column: nid
        Columns:
        	nid, vocab nid (size 20738)
        	cat, vocab llama (size 32000), max length 8
        	subcat, vocab llama (size 32000), max length 5
        	title, vocab llama (size 32000), max length 20
        	abs, vocab llama (size 32000), max length 50

[00:00:07] |Controller| Selected Item Encoder: LlamaOperator
[00:00:07] |Controller| Selected User Encoder: AdaOperator
[00:00:07] |Controller| Selected Predictor: DotPredictor
[00:00:07] |Controller| Use Negative Sampling: False
[00:00:07] |Controller| Use Item Content: True
[00:00:07] |EmbeddingHub| load pretrained embedding llama of torch.Size([32000, 4096])
Returning a hub: <loader.data_hub.DataHub object at 0x15366ea44950>
<loader.data_hub.DataHub object at 0x15366ea44950>
[00:00:07] |EmbeddingHub| skip col history
[00:00:07] |EmbeddingHub| create vocab __cat_inputer_special_ids (3, 4096)
[00:00:07] |EmbeddingHub| create vocab __flatten_seq_special_ids (4, 4096)
<loader.data_hub.DataHub object at 0x1536f2afe550>
[00:00:07] |EmbeddingHub| build mapping title -> llama
[00:00:07] |EmbeddingHub| load frozen vocab: llama torch.Size([32000, 4096])
[00:00:07] |EmbeddingHub| keep transform size 4096
[00:00:07] |EmbeddingHub| build mapping cat -> llama
Returning a hub: <loader.data_hub.DataHub object at 0x15366ea44950>
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:10,  2.10s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:04<00:08,  2.11s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:06<00:06,  2.12s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:08<00:04,  2.23s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:10<00:02,  2.15s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:11<00:00,  1.56s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:11<00:00,  1.86s/it]
trainable params: 16,777,216 || all params: 6,493,048,832 || trainable%: 0.2584
  0%|          | 0/20738 [00:00<?, ?it/s]  6%|▌         | 1187/20738 [00:00<00:01, 11864.24it/s] 12%|█▏        | 2465/20738 [00:00<00:01, 12400.60it/s] 18%|█▊        | 3722/20738 [00:00<00:01, 12477.41it/s] 24%|██▍       | 4984/20738 [00:00<00:01, 12530.88it/s] 30%|███       | 6245/20738 [00:00<00:01, 12558.12it/s] 36%|███▌      | 7511/20738 [00:00<00:01, 12589.98it/s] 42%|████▏     | 8770/20738 [00:00<00:00, 12578.57it/s] 48%|████▊     | 10028/20738 [00:00<00:00, 12573.51it/s] 54%|█████▍    | 11296/20738 [00:00<00:00, 12604.59it/s] 61%|██████    | 12557/20738 [00:01<00:00, 12599.42it/s] 67%|██████▋   | 13823/20738 [00:01<00:00, 12616.72it/s] 73%|███████▎  | 15085/20738 [00:01<00:00, 12587.48it/s] 79%|███████▉  | 16344/20738 [00:01<00:00, 12584.31it/s] 85%|████████▍ | 17609/20738 [00:01<00:00, 12602.55it/s] 91%|█████████ | 18870/20738 [00:01<00:00, 12599.60it/s] 97%|█████████▋| 20140/20738 [00:01<00:00, 12629.17it/s]100%|██████████| 20738/20738 [00:01<00:00, 12567.39it/s]
Item cache built with 20738 items.
First few items in item_cache:
item_cache[0]: {'input_ids': OrderedDict([('natural_cat', tensor([10130,  4274, 29901,   529,  3257, 29958, 19259,   294,  5815,   722,
        27408,   972, 27462,  1655,   529,  7320, 29958,   413,  5632, 29875,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0]))]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
item_cache[1]: {'input_ids': OrderedDict([('natural_cat', tensor([10130,  4274, 29901,   529,  3257, 29958, 25472,  7828,  9149,   260,
        29926,  2016, 15187,   529,  7320, 29958,  1090,  8948,  1076,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0]))]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
item_cache[2]: {'input_ids': OrderedDict([('natural_cat', tensor([10130,  4274, 29901,   529,  3257, 29958, 15533,   264,  8135,   348,
          285, 29891,  2267,   474,   317, 30077,  5740, 29926, 29891,   808,
        29923,   529,  7320, 29958,  7980,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0]))]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
item_cache[3]: {'input_ids': OrderedDict([('natural_cat', tensor([10130,  4274, 29901,   529,  3257, 29958,  8591, 11795, 11340, 29873,
          357,  3548, 19606,   529,  7320, 29958,  7098, 29882,  2447,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0]))]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
item_cache[4]: {'input_ids': OrderedDict([('natural_cat', tensor([10130,  4274, 29901,   529,  3257, 29958,  8045,   495, 14167, 29901,
          379, 29894,  1398,  3679,   604,   767,  3477,   307, 29973,   529,
         7320, 29958,  7916, 29918,   468, 29918, 29879,  8807,   440,     0,
            0,     0,     0,     0,     0,     0,     0]))]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])}
self.use_neg_sampling False
[00:00:23] |Worker| {'index': 0, 'imp': 0, 'uid': 11894, 'nid': 18909, 'click': 0, 'history': array([20738, 20738, 20738, 20738, 20738, 20738, 20738, 20738, 20738,
       20738, 20738, 15607, 15948, 15928, 15902, 16253, 16268, 16273,
       16267, 16259, 15898, 1648, 17022, 17303, 17331, 17306, 17286,
       17808, 17798, 17792], dtype=object)}
Some nids in [20738, 20738, 20738, 20738, 20738, 20738, 20738, 20738, 20738, 20738, 20738, 15607, 15948, 15928, 15902, 16253, 16268, 16273, 16267, 16259, 15898, 1648, 17022, 17303, 17331, 17306, 17286, 17808, 17798, 17792] are not in item_cache when rebuilding clicks
Invalid nids: [20738, 20738, 20738, 20738, 20738, 20738, 20738, 20738, 20738, 20738, 20738]
[00:00:23] |Worker| {
    "history": {
        "input_ids": {
            "natural_cat": "tensor([19, 37], dtype=torch.int64)"
        },
        "attention_mask": "tensor([19, 37], dtype=torch.int64)"
    },
    "nid": {
        "input_ids": {
            "natural_cat": "tensor([1, 37], dtype=torch.int64)"
        },
        "attention_mask": "tensor([1, 37], dtype=torch.int64)"
    },
    "click": "int",
    "imp": "int",
    "uid": "int",
    "__clicks_mask__": "tensor([30], dtype=torch.int64)"
}
[00:00:23] |Worker| split item pretrained encoder parameters
[00:00:23] |Worker| pretrained lr: 1e-05
[00:00:23] |Worker| other lr: 0.0001
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:23] |Legommender| [P] item_encoder.transformer.base_model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.Size([32, 4096])
[00:00:24] |Legommender| [P] item_encoder.transformer.base_model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 32])
[00:00:24] |Legommender| [N] embedding_table.__cat_inputer_special_ids.weight torch.Size([3, 4096])
[00:00:24] |Legommender| [N] embedding_table.__flatten_seq_special_ids.weight torch.Size([4, 4096])
[00:00:24] |Legommender| [N] user_encoder.additive_attention.encoder.0.weight torch.Size([256, 64])
[00:00:24] |Legommender| [N] user_encoder.additive_attention.encoder.0.bias torch.Size([256])
[00:00:24] |Legommender| [N] user_encoder.additive_attention.encoder.2.weight torch.Size([1, 256])
[00:00:24] |Legommender| [N] item_encoder.linear.weight torch.Size([64, 4096])
[00:00:24] |Legommender| [N] item_encoder.linear.bias torch.Size([64])
[00:00:24] |Legommender| [N] item_encoder.additive_attention.encoder.0.weight torch.Size([64, 64])
[00:00:24] |Legommender| [N] item_encoder.additive_attention.encoder.0.bias torch.Size([64])
[00:00:24] |Legommender| [N] item_encoder.additive_attention.encoder.2.weight torch.Size([1, 64])
  0%|          | 0/80805 [00:00<?, ?it/s]  0%|          | 0/80805 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/gpfs/home3/scur1569/Legommenders/worker.py", line 489, in <module>
    worker.run()
  File "/gpfs/home3/scur1569/Legommenders/worker.py", line 441, in run
    epoch = self.train_runner()
            ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home3/scur1569/Legommenders/worker.py", line 384, in train_runner
    return self.train()
           ^^^^^^^^^^^^
  File "/gpfs/home3/scur1569/Legommenders/worker.py", line 168, in train
    for step, batch in enumerate(tqdm(loader, disable=self.disable_tqdm)):
  File "/home/scur1569/.conda/envs/dire_tokenize/lib/python3.11/site-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
  File "/home/scur1569/.conda/envs/dire_tokenize/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/home/scur1569/.conda/envs/dire_tokenize/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
    return self._process_data(data)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1569/.conda/envs/dire_tokenize/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
    data.reraise()
  File "/home/scur1569/.conda/envs/dire_tokenize/lib/python3.11/site-packages/torch/_utils.py", line 722, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/scur1569/.conda/envs/dire_tokenize/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1569/.conda/envs/dire_tokenize/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1569/.conda/envs/dire_tokenize/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 277, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1569/.conda/envs/dire_tokenize/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 129, in collate
    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1569/.conda/envs/dire_tokenize/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 129, in <dictcomp>
    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1569/.conda/envs/dire_tokenize/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 129, in collate
    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1569/.conda/envs/dire_tokenize/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 129, in <dictcomp>
    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1569/.conda/envs/dire_tokenize/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 129, in collate
    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1569/.conda/envs/dire_tokenize/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 129, in <dictcomp>
    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1569/.conda/envs/dire_tokenize/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 121, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur1569/.conda/envs/dire_tokenize/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 173, in collate_tensor_fn
    out = elem.new(storage).resize_(len(batch), *list(elem.size()))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Trying to resize storage that is not resizable

srun: error: gcn53: task 0: Exited with exit code 1
srun: Terminating StepId=6639131.0

JOB STATISTICS
==============
Job ID: 6639131
Cluster: snellius
User/Group: scur1569/scur1569
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 00:13:48 core-walltime
Job Wall-clock time: 00:00:46
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 120.00 GB (120.00 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
