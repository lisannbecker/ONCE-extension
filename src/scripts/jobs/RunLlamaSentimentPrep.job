#!/bin/bash

#SBATCH --partition=gpu_mig
#SBATCH --gpus=1
#SBATCH --job-name=PrepSent
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --time=01:00:00
#SBATCH --output=job_outputs/Training/Sentiment/Prep_FastFormer_llama7b_sentiment%A.out

module purge
module load 2022
module load Anaconda3/2022.05

cd $HOME/ONCE-extension/src/lib/Legommenders/

source activate once

#Fastformer
# Pre-training fast former
srun python worker.py --embed config/embed/llama-token.yaml --model config/model/llm/llama-fastformer-sentiment.yaml --exp config/exp/llama-split-sentiment.yaml --data config/data/eb-nerd-sentiment.yaml --version small --llm_ver 7b --hidden_size 64 --layer 0 --lora 0 --fast_eval 0 --embed_hidden_size 4096 --page_size 8 --cuda -1

# Pre-training NAML
srun python worker.py --embed config/embed/llama-token.yaml --model config/model/llm/llama-naml-sentiment.yaml --exp config/exp/llama-split-sentiment.yaml --data config/data/eb-nerd-sentiment.yaml --version small --llm_ver 7b --hidden_size 64 --layer 0 --lora 0 --fast_eval 0 --embed_hidden_size 4096 --page_size 8 --cuda -1

# Pre-training NRMS
srun python worker.py --embed config/embed/llama-token.yaml --model config/model/llm/llama-nrms-sentiment.yaml --exp config/exp/llama-split-sentiment.yaml --data config/data/eb-nerd-sentiment.yaml --version small --llm_ver 7b --hidden_size 64 --layer 0 --lora 0 --fast_eval 0 --embed_hidden_size 4096 --page_size 8 --cuda -1